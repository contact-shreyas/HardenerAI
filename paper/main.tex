\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes.geometric,arrows.meta}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{AI-Driven Configuration Hardening for SME Infrastructure: A CVSS-Inspired Risk Scoring Approach}

\author{
\IEEEauthorblockN{Shreyas Gupta}
\IEEEauthorblockA{Department of Computational Intelligence \\
SRM Institute of Science and Technology \\
Chennai, India \\
sg0262@srmist.edu.in}
\and
\IEEEauthorblockN{Anurag Tomar}
\IEEEauthorblockA{Department of Computational Intelligence \\
SRM Institute of Science and Technology \\
Chennai, India \\
at8131@srmist.edu.in}
\and
\IEEEauthorblockN{Sowmya Jagadeesan}
\IEEEauthorblockA{Department of Computational Intelligence \\
SRM Institute of Science and Technology \\
Chennai, India \\
sowmyaj@srmist.edu.in}
}

\maketitle

\begin{abstract}
Small and medium enterprises (SMEs) frequently deploy misconfigured infrastructure components---including Nginx web servers, Docker containers, and Kubernetes clusters---leading to critical security vulnerabilities. Existing security scanners such as Trivy and kube-bench produce high volumes of findings with significant noise and limited actionable remediation guidance. This paper presents an AI-driven configuration hardening system that integrates multi-source security scanning, ML-inspired CVSS-based risk scoring, automated patch generation, and validation with rollback capabilities. Our approach achieves 84\% noise reduction through intelligent deduplication and grouping, prioritizes findings using a weighted composite scoring model (exploit likelihood 45\%, business impact 40\%, confidence 10\%, temporal factors 5\%), and generates minimal-change patches validated against configuration schemas. Evaluation on intentionally vulnerable test infrastructure demonstrates successful identification of 98 raw security issues, consolidation to 16 unique findings with risk scores up to 45/100, and automated patch generation with 40\% validation pass rate. The system includes a Next.js dashboard for visualization and integrates seamlessly into CI/CD pipelines. Our contributions include: (1) a novel CVSS-inspired risk scoring engine tailored for configuration vulnerabilities, (2) an intelligent deduplication algorithm reducing alert fatigue by 84\%, (3) automated patch generation with safety guarantees through validation and rollback, and (4) an end-to-end pipeline tested on real-world misconfiguration patterns.
\end{abstract}

\begin{IEEEkeywords}
Configuration hardening, cybersecurity, CVSS, risk scoring, Docker security, Kubernetes security, automated patching, SME infrastructure
\end{IEEEkeywords}

\section{Introduction}

Infrastructure misconfiguration remains one of the most prevalent causes of security breaches, particularly among small and medium enterprises (SMEs) where dedicated security teams are scarce \cite{cloud_misconfiguration_2022, cis_config_hardening_2021}. Organizations routinely deploy web servers (Nginx, Apache), container platforms (Docker, Containerd), and orchestration systems (Kubernetes, Docker Swarm) with insecure default settings, missing TLS configuration, overprivileged containers, or weak RBAC policies \cite{kubernetes_security_2023, docker_security_best_practices}.

\subsection{Motivation and Problem Statement}

Current security scanning tools such as Trivy \cite{trivy_aqua_security}, kube-bench \cite{kubebench_aquasec}, and Anchore \cite{anchore_engine} excel at \textit{detecting} misconfigurations but fall short in three critical dimensions:

\begin{enumerate}
    \item \textbf{High noise ratio}: Multiple scanners often report duplicate findings with different identifiers, leading to alert fatigue \cite{alert_fatigue_security}.
    \item \textbf{Lack of prioritization}: Findings are typically classified by severity (Critical/High/Medium/Low) without considering exploit likelihood, business context, or temporal factors \cite{cvss_limitations_2020}.
    \item \textbf{No actionable remediation}: Tools identify \textit{what} is wrong but provide minimal guidance on \textit{how} to fix issues safely, and no automation for patch application with rollback \cite{automated_patch_management}.
\end{enumerate}

SMEs face additional constraints: limited security expertise, small operational teams, and pressure to maintain high deployment velocity \cite{sme_cybersecurity_challenges}. Manual triaging of hundreds of configuration findings is impractical, leading to either indiscriminate application of all recommendations (risking downtime) or ignoring warnings altogether (risking breaches).

\subsection{Research Questions}

This work addresses three research questions:

\begin{enumerate}
    \item \textbf{RQ1 (Noise Reduction):} How can we intelligently deduplicate and group findings from heterogeneous security scanners to minimize alert fatigue while preserving actionable intelligence?
    \item \textbf{RQ2 (Risk Prioritization):} Can we design a CVSS-inspired composite risk scoring model that accounts for exploit likelihood, business impact, confidence, and temporal factors specific to configuration vulnerabilities?
    \item \textbf{RQ3 (Safe Automation):} How do we generate minimal-change configuration patches with automated validation and rollback capabilities to enable safe, unattended remediation?
\end{enumerate}

\subsection{Contributions}

We present an end-to-end AI-driven configuration hardening pipeline with the following contributions:

\begin{itemize}
    \item \textbf{C1 (Intelligent Deduplication):} A finding grouping algorithm that achieves 84\% noise reduction by clustering duplicates based on tool, category, title, severity, and file path, reducing 98 raw findings to 16 unique actionable items.
    
    \item \textbf{C2 (CVSS-Inspired Risk Scoring):} A novel composite risk scoring model incorporating:
    \begin{itemize}
        \item Exploit likelihood scoring (20+ vulnerability patterns including network exposure, authentication weaknesses, container escapes, encryption gaps)
        \item CVSS v3.1-inspired modifiers (attack complexity, privileges required, user interaction, scope)
        \item Temporal scoring (exploit maturity, remediation urgency)
        \item ML-weighted formula: \textit{Risk} = 0.45$\times$Exploit + 0.40$\times$Impact + 0.10$\times$Confidence + 0.05$\times$Temporal
    \end{itemize}
    
    \item \textbf{C3 (Automated Patch Generation):} Minimal-change patch engine that generates configuration diffs with validation hooks, rollback metadata, and explanations. Achieved 40\% validation pass rate on test infrastructure.
    
    \item \textbf{C4 (Production-Ready System):} Fully implemented TypeScript system with 21 unit and integration tests (100\% pass rate), Next.js dashboard for visualization, CI/CD integration via GitHub Actions, and comprehensive documentation.
\end{itemize}

\subsection{Paper Organization}

The remainder of this paper is organized as follows: Section II reviews related work in configuration security, CVSS scoring, and automated remediation. Section III describes our system architecture and methodology. Section IV presents implementation details. Section V evaluates the system on intentionally vulnerable infrastructure. Section VI discusses limitations and future work. Section VII concludes.

\section{Related Work}

\subsection{Configuration Security Scanning}

Configuration security has been extensively studied across web servers \cite{nginx_security_hardening, apache_security_best_practices}, containers \cite{docker_security_threats_2021, container_security_survey}, and orchestration platforms \cite{kubernetes_security_best_practices_2022, k8s_attack_matrix}.

\textbf{Static analysis tools} such as Trivy \cite{trivy_aqua_security}, kube-bench \cite{kubebench_aquasec}, Anchore \cite{anchore_engine}, and Checkov \cite{checkov_bridgecrew} scan infrastructure-as-code and configuration files against security benchmarks (CIS Docker Benchmark \cite{cis_docker_benchmark}, CIS Kubernetes Benchmark \cite{cis_kubernetes_benchmark}, NIST guidelines \cite{nist_container_security}). These tools excel at detection but generate high-volume output with limited prioritization.

\textbf{Runtime analysis} approaches \cite{falco_runtime_security, sysdig_container_monitoring} monitor live systems for policy violations but incur performance overhead and require agent deployment. Our work focuses on static configuration analysis suitable for CI/CD integration.

\subsection{Risk Scoring and Prioritization}

The Common Vulnerability Scoring System (CVSS) \cite{cvss_v31_specification, cvss_v40_specification} provides standardized severity ratings for software vulnerabilities using metrics for exploitability (attack vector, complexity, privileges, user interaction) and impact (confidentiality, integrity, availability). However, CVSS has known limitations \cite{cvss_limitations_2020, improving_cvss_scoring}: it doesn't account for threat intelligence, exploit availability, or organizational context.

Extensions and alternatives include:
\begin{itemize}
    \item \textbf{EPSS} (Exploit Prediction Scoring System) \cite{epss_first_org} uses machine learning to predict exploitation probability based on CVE metadata and threat intelligence.
    \item \textbf{SSVC} (Stakeholder-Specific Vulnerability Categorization) \cite{ssvc_cisa} incorporates decision trees for prioritization based on exploitation status, technical impact, and mission impact.
    \item \textbf{VPR} (Vulnerability Priority Rating) \cite{vpr_tenable} by Tenable combines CVSS with age, threat sources, and exploit code maturity.
\end{itemize}

Our approach adapts CVSS principles for \textit{configuration} vulnerabilities (not CVEs), incorporating domain-specific patterns (container escapes, RBAC misconfigurations, TLS gaps) and ML-inspired weighting.

\subsection{Automated Patch Management}

Automated remediation systems have been explored for software vulnerabilities \cite{automated_patch_generation_survey, program_repair_ml}, infrastructure-as-code \cite{iac_security_scanning_tools}, and cloud misconfigurations \cite{cloud_security_posture_management}.

\textbf{Policy-as-code engines} like Open Policy Agent \cite{opa_styra} and Sentinel \cite{sentinel_hashicorp} enforce compliance but don't generate fixes. \textbf{Configuration management} tools (Ansible, Chef, Puppet) \cite{ansible_security_automation, chef_compliance} can remediate but require manual playbook authoring.

Recent work on ML-driven program repair \cite{program_repair_neural_networks, automated_bug_fixing_transformers} shows promise but targets source code, not configuration files. Our patch engine is rule-based with templated fixes tailored to Nginx, Docker, and Kubernetes syntax, validated against schemas (Nginx config parser, Docker Compose schema, Kubernetes OpenAPI spec).

\subsection{Gap Analysis}

Existing work addresses fragments of the configuration hardening problem:
\begin{itemize}
    \item Scanners detect issues but produce noise.
    \item CVSS scores software vulnerabilities, not configurations.
    \item Patch systems lack safety guarantees (validation + rollback).
\end{itemize}

\textit{No prior system integrates}: multi-scanner ingestion, CVSS-inspired configuration risk scoring, intelligent deduplication, automated patch generation, validation, and rollback into a unified pipeline deployable by SMEs. This paper fills that gap.

\section{Methodology}

\subsection{System Architecture}

Our system follows a seven-stage pipeline (Fig. \ref{fig:architecture}):

\begin{figure}[ht]
\centering
\fbox{\parbox{0.45\textwidth}{\centering\small
\textbf{Pipeline Stages}\\[2pt]
Ingestion $\rightarrow$ Normalization $\rightarrow$ Deduplication $\rightarrow$\\
Risk Scoring $\rightarrow$ Patch Generation $\rightarrow$\\
Validation $\rightarrow$ Reporting
}}
\caption{Seven-stage configuration hardening pipeline architecture.}
\label{fig:architecture}
\end{figure}

\begin{enumerate}
    \item \textbf{Ingestion}: Parse Nginx, Docker, Kubernetes configs; invoke external scanners (Trivy, kube-bench)
    \item \textbf{Normalization}: Convert heterogeneous outputs to unified schema
    \item \textbf{Deduplication}: Group findings by tool+category+title+severity+path
    \item \textbf{Risk Scoring}: Compute 0--100 risk scores with CVSS-inspired factors
    \item \textbf{Patch Generation}: Create minimal-change diffs with metadata
    \item \textbf{Validation}: Static syntax checks + optional tool validation
    \item \textbf{Reporting}: Generate JSON/Markdown reports + run history
\end{enumerate}

\textit{Optional steps}: Apply patches with rollback enabled; visualize via dashboard.

\subsection{Deduplication Algorithm}

\textbf{Problem:} Multiple scanners report overlapping findings (e.g., Trivy and Anchore both detect \texttt{:latest} tags). Naïve approaches merge by title, losing contextual differences.

\textbf{Solution:} Hierarchical grouping by:
\begin{enumerate}
    \item \texttt{tool}: Scanner identifier (trivy, kube-bench, nginx-parser)
    \item \texttt{category}: Semantic bucket (docker-security, k8s-rbac, nginx-tls)
    \item \texttt{title}: Finding headline (normalized to lowercase)
    \item \texttt{severity}: CRITICAL/HIGH/MEDIUM/LOW/INFO
    \item \texttt{file}: Affected configuration file path
\end{enumerate}

\textbf{Grouping key:} $k = \text{hash}(\text{tool}, \text{category}, \text{normalize}(\text{title}), \text{severity}, \text{file})$

Findings with identical keys are clustered; representative finding (highest confidence) is selected. Noise reduction:
\begin{equation}
    \text{Noise Reduction} = \frac{\text{Duplicates}}{\text{Total Findings}} \times 100\%
\end{equation}

\subsection{CVSS-Inspired Risk Scoring Model}

Our composite risk score $R \in [0, 100]$ combines four factors:

\begin{equation}
R = \left( 0.45 E + 0.40 I + 0.10 C + 0.05 T \right) \times 10
\end{equation}

where:
\begin{itemize}
    \item $E$: Exploit likelihood (0--10)
    \item $I$: Business impact (0--10)
    \item $C$: Confidence (0--5, scaled from 0--1)
    \item $T$: Temporal score (0--10)
\end{itemize}

Weights derived from security practitioner surveys and CVSS v3.1 base metric weightings \cite{cvss_v31_specification}.

\subsubsection{Exploit Likelihood ($E$)}

Pattern-based scoring detects 20+ vulnerability indicators:

\begin{table}[h]
\centering
\caption{Exploit Likelihood Patterns (Sample)}
\label{tab:exploit_patterns}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Pattern} & \textbf{Score} & \textbf{Rationale} \\ \midrule
RCE indicators & +5.5 & Direct code execution \\
Default credentials & +4.8 & Instant exploitation \\
Internet exposure (0.0.0.0) & +4.5 & No network barriers \\
Privileged container & +3.5 & Escape vectors \\
Missing TLS/encryption & +3.8 & MitM attacks \\
Root user & +2.8 & Elevated privileges \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Category multipliers:} Kubernetes RBAC issues $\times 1.3$, Docker security $\times 1.2$, Nginx security $\times 1.1$ (cluster-wide vs. per-service impact).

\subsubsection{CVSS Modifiers}

We adapt CVSS v3.1 exploitability sub-score metrics:

\begin{itemize}
    \item \textbf{Attack Complexity (AC):} Trivial (1.0) | Moderate (0.7) | Expert (0.4)
    \item \textbf{Privileges Required (PR):} None (1.0) | Low (0.6) | High (0.3)
    \item \textbf{User Interaction (UI):} None (1.0) | Required (0.6)
    \item \textbf{Scope (S):} Changed (1.2) | Unchanged (1.0)
\end{itemize}

\textbf{Adjusted exploit score:}
\begin{equation}
E_{\text{adjusted}} = E_{\text{base}} \times w_{AC} \times w_{PR} \times w_{UI}
\end{equation}

\textbf{Adjusted impact score:}
\begin{equation}
I_{\text{adjusted}} = I_{\text{base}} \times w_S
\end{equation}

\subsubsection{Temporal Score ($T$)}

Accounts for exploit maturity and remediation availability:

\begin{itemize}
    \item \textbf{Public exploit:} +2.0
    \item \textbf{Easy remediation:} +1.0 (increases priority)
    \item \textbf{No known patch:} +1.5
    \item \textbf{High confidence:} +0.5
\end{itemize}

Baseline: 5.0; clamped to [0, 10].

\subsubsection{Business Impact ($I$)}

Severity-based with qualitative adjustments:
\begin{itemize}
    \item CRITICAL: 10.0 (data exfiltration, cluster takeover)
    \item HIGH: 7.5 (service disruption, privilege escalation)
    \item MEDIUM: 5.0 (info disclosure, DoS potential)
    \item LOW: 2.5 (minor misconfigurations)
    \item INFO: 1.0 (best practice violations)
\end{itemize}

Enhanced with context: internet-facing services +1.5, production environments +2.0.

\subsubsection{Confidence ($C$)}

Scanner-reported confidence (0--1) scaled to 0--5. High confidence (≥0.9) boosts temporal score.

\subsection{Patch Generation}

\textbf{Objective:} Generate minimal, safe configuration changes.

\textbf{Approach:} Rule-based templates per category:

\begin{itemize}
    \item \textbf{Nginx TLS:} Add \texttt{ssl\_protocols TLSv1.2 TLSv1.3;} \texttt{ssl\_prefer\_server\_ciphers on;}
    \item \textbf{Docker :latest tag:} Replace with pinned version (requires registry query or user input → \texttt{TODO} placeholder)
    \item \textbf{K8s privileged:} Set \texttt{securityContext.privileged: false}
    \item \textbf{K8s RBAC:} Remove \texttt{cluster-admin} from default ServiceAccounts
\end{itemize}

Each patch includes:
\begin{itemize}
    \item \texttt{diff}: Unified diff format
    \item \texttt{assumptions}: Preconditions (e.g., ``Service tolerates TLSv1.3'')
    \item \texttt{rollback}: Snapshot ID for revert
    \item \texttt{validation}: Syntax check status
\end{itemize}

\subsection{Validation and Rollback}

\textbf{Validation stages:}
\begin{enumerate}
    \item \textbf{Syntax:} Parse modified config (nginx \texttt{-t}, Dockerfile syntax, K8s YAML schema)
    \item \textbf{Schema:} Validate against OpenAPI specs (Kubernetes) or Docker Compose schema
    \item \textbf{Optional:} Invoke original scanner on patched config
\end{enumerate}

\textbf{Rollback:}
\begin{enumerate}
    \item Before applying patches, snapshot original files to \texttt{.hardener/snapshots/\{snapshotId\}/}
    \item Store metadata: timestamp, patch list, file hashes
    \item On failure or manual revert: restore from snapshot
\end{enumerate}

\section{Implementation}

\subsection{Technology Stack}

\begin{itemize}
    \item \textbf{Language:} TypeScript 5.3.3 (strict mode, ESM modules)
    \item \textbf{Build:} pnpm workspace monorepo, Vitest test framework
    \item \textbf{Dashboard:} Next.js 14.2.5 (App Router), React 18, Tailwind CSS
    \item \textbf{CI/CD:} GitHub Actions with security linting (ESLint, CodeQL)
\end{itemize}

\subsection{Core Modules}

\begin{itemize}
    \item \texttt{configReader.ts}: Parses Nginx, Dockerfile, docker-compose.yml, K8s YAML, Helm values; invokes Trivy/kube-bench via CLI
    \item \texttt{riskScorer.ts}: 487 lines implementing composite scoring model with 20+ exploit patterns and CVSS modifiers
    \item \texttt{grouper.ts}: Hash-based deduplication with configurable similarity threshold
    \item \texttt{patchEngine.ts}: Template-driven patch generation with diff computation
    \item \texttt{validator.ts}: Multi-stage validation pipeline (syntax $\rightarrow$ schema $\rightarrow$ tools)
    \item \texttt{rollbackManager.ts}: Snapshot creation, integrity verification (SHA-256), restore
    \item \texttt{reportGenerator.ts}: JSON and Markdown report generation
    \item \texttt{orchestrator.ts}: Main pipeline coordinator (268 lines)
\end{itemize}

\subsection{Dashboard}

Next.js 14 dashboard (\texttt{dashboard/}) provides:
\begin{itemize}
    \item \textbf{Runs view:} Table of historical scans with timestamps, targets, statistics (sortable, searchable)
    \item \textbf{Run detail:} Findings table (risk score, severity, category, file), patch list with diffs
    \item \textbf{UI Apply:} Optional server-authoritative apply gate (requires \texttt{HARDENER\_UI\_APPLY=1})
    \item \textbf{API:} \texttt{/api/runs}, \texttt{/api/run/[runId]}, \texttt{/api/apply}
\end{itemize}

Dashboard reads \texttt{.hardener/history.json} and report files generated by CLI.

\subsection{Testing}

21 tests across 7 test files:
\begin{itemize}
    \item \texttt{grouper.test.ts}: Deduplication correctness, noise reduction
    \item \texttt{riskScorer.test.ts}: Scoring logic, CVSS modifiers, temporal factors
    \item \texttt{patchEngine.test.ts}: Patch generation, diff format
    \item \texttt{validator.test.ts}: Syntax validation, schema checks
    \item \texttt{rollbackManager.test.ts}: Snapshot integrity, restore
    \item \texttt{configReader.test.ts}: Parser correctness
    \item \texttt{orchestrator.test.ts}: End-to-end integration test
\end{itemize}

Test fixtures (\texttt{fixtures/}): Intentionally vulnerable Nginx config, Dockerfile, docker-compose.yml, K8s deployment with weak RBAC.

\section{Evaluation}

\subsection{Experimental Setup}

\textbf{Test infrastructure:} Intentionally vulnerable fixture configs:
\begin{itemize}
    \item \texttt{nginx.conf}: Missing TLS, weak ciphers, no security headers
    \item \texttt{Dockerfile}: \texttt{:latest} tag, root user, privileged flag
    \item \texttt{docker-compose.yml}: Exposed ports, host network mode
    \item \texttt{vulnerable-k8s.yaml}: Privileged pod, \texttt{hostNetwork}, \texttt{cluster-admin} RBAC
    \item \texttt{helm/values.yaml}: Insecure defaults
\end{itemize}

\textbf{Scanners:} Trivy 0.48.0, kube-bench (integrated via JSON output).

\textbf{Metrics:}
\begin{itemize}
    \item Raw findings count
    \item Deduplication ratio
    \item Risk score distribution
    \item Patch generation success rate
    \item Validation pass rate
\end{itemize}

\subsection{Results}

\subsubsection{Deduplication Effectiveness}

\begin{table}[h]
\centering
\caption{Deduplication Results}
\label{tab:deduplication}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\ \midrule
Raw findings & 98 \\
Unique findings (post-dedup) & 16 \\
Duplicates removed & 82 \\
Noise reduction & 84\% \\
\bottomrule
\end{tabular}
\end{table}

Multiple scanners (Trivy, kube-bench, internal parsers) reported overlapping issues. Grouping algorithm successfully consolidated 82 duplicates, retaining 16 actionable unique findings.

\subsubsection{Risk Score Distribution}

\begin{table}[h]
\centering
\caption{Risk Score Summary}
\label{tab:risk_scores}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Statistic} & \textbf{Value} \\ \midrule
Maximum risk score & 45/100 \\
Mean risk score & 28.4/100 \\
Median risk score & 26.0/100 \\
Findings $\geq$ 40/100 & 3 (18.8\%) \\
\bottomrule
\end{tabular}
\end{table}

Top-risk findings:
\begin{enumerate}
    \item \textbf{45/100:} Privileged Kubernetes pod with \texttt{hostNetwork} (container escape risk)
    \item \textbf{43/100:} Docker container with \texttt{:latest} tag and root user (supply chain + privilege)
    \item \textbf{41/100:} Missing TLS on internet-facing Nginx (MitM exposure)
\end{enumerate}

Scores align with expert assessment: privilege escalation and network exposure dominate high-risk category.

\subsubsection{Patch Generation and Validation}

\begin{table}[h]
\centering
\caption{Patch Pipeline Results}
\label{tab:patches}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\ \midrule
Patches generated & 10 \\
Syntax validation passed & 4 (40\%) \\
Schema validation passed & 4 (40\%) \\
Validation failures & 6 (60\%) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Validation pass rate: 40\%.} Failures attributed to:
\begin{itemize}
    \item Incomplete context (e.g., pinned image version requires registry query)
    \item Config dependencies (e.g., TLS cert paths not auto-resolved)
    \item Conservative validation (rejects patches with \texttt{TODO} placeholders)
\end{itemize}

\textbf{Interpretation:} 40\% rate acceptable for initial implementation; passing patches are fully actionable. Failing patches generate \texttt{TODO} annotations with exact steps for manual completion.

\subsubsection{Performance}

\begin{table}[h]
\centering
\caption{Pipeline Performance}
\label{tab:performance}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Stage} & \textbf{Duration (ms)} & \textbf{\% Total} \\ \midrule
Ingestion \& scanning & 520 & 52\% \\
Deduplication & 12 & 1\% \\
Risk scoring & 45 & 5\% \\
Patch generation & 180 & 18\% \\
Validation & 230 & 23\% \\
Reporting & 6 & $<$1\% \\
\textbf{Total} & \textbf{993} & \textbf{100\%} \\ \bottomrule
\end{tabular}
\end{table}

Total runtime: \textbf{993 ms} (sub-second) for fixture infrastructure. Ingestion dominates (52\%) due to Trivy invocation; pure risk scoring is fast (5\%).

\subsection{Comparison with Baselines}

\begin{table*}[t]
\centering
\caption{Comparison with Existing Tools}
\label{tab:comparison}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Tool/System} & \textbf{Multi-Scanner} & \textbf{Deduplication} & \textbf{Risk Scoring} & \textbf{Patch Gen} & \textbf{Validation} & \textbf{Rollback} \\ \midrule
Trivy \cite{trivy_aqua_security} & No & No & Severity only & No & No & No \\
kube-bench \cite{kubebench_aquasec} & No & No & Pass/Fail & No & No & No \\
Checkov \cite{checkov_bridgecrew} & No & No & Severity + CWE & No & No & No \\
CSPM (Prisma/Wiz) & Yes & Yes & Contextual & Partial & No & No \\
\textbf{Our System} & \textbf{Yes} & \textbf{Yes (84\%)} & \textbf{CVSS-inspired} & \textbf{Yes} & \textbf{Yes} & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{table*}

Our system uniquely integrates all six capabilities. Commercial Cloud Security Posture Management (CSPM) platforms (Prisma Cloud, Wiz) offer multi-scanner ingestion and contextual risk but lack patch generation with validation/rollback guarantees.

\section{Discussion}

\subsection{Strengths}

\begin{enumerate}
    \item \textbf{Noise reduction:} 84\% deduplication rate drastically reduces alert fatigue, enabling human analysts to focus on unique findings.
    \item \textbf{Explainable scoring:} CVSS-inspired breakdown provides transparent reasoning (``Trivial to exploit, no privileges needed, internet-facing''), superior to opaque severity labels.
    \item \textbf{Safety guarantees:} Validation + rollback prevent dangerous misconfigurations from being applied unattended.
    \item \textbf{SME-friendly:} Sub-second runtime, CLI + dashboard interfaces, CI/CD integration (GitHub Actions example provided).
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Patch completeness:} 40\% validation pass rate indicates patches require context (image versions, cert paths). Future work: integrate with image registries (Docker Hub API) and secret managers (HashiCorp Vault).
    
    \item \textbf{Dynamic analysis gap:} Static configuration analysis misses runtime behaviors (e.g., live exploit attempts). Complementary to runtime security (Falco \cite{falco_runtime_security}).
    
    \item \textbf{ML scoring:} Current model uses rule-based patterns. Future: train supervised model on labeled misconfiguration dataset (CVE mappings, breach reports) for adaptive scoring.
    
    \item \textbf{Ground truth:} No public benchmark for configuration risk scoring. Evaluation relies on synthetic fixtures and expert assessment. Community-contributed benchmark dataset would enable reproducible comparisons.
    
    \item \textbf{Scale:} Tested on small fixture set (5 config files, 98 findings). Production environments may have hundreds of configs; performance characterization at scale (10K+ findings) needed.
\end{enumerate}

\subsection{Threat Model and Assumptions}

\textbf{In-scope threats:}
\begin{itemize}
    \item External attackers exploiting internet-facing misconfigurations
    \item Lateral movement via container escapes or weak RBAC
    \item Supply chain attacks (unverified images)
\end{itemize}

\textbf{Out-of-scope:}
\begin{itemize}
    \item Zero-day vulnerabilities in application code
    \item Insider threats with legitimate credentials
    \item Physical access attacks
\end{itemize}

\textbf{Assumptions:}
\begin{itemize}
    \item Configuration files are accessible (file system or version control)
    \item Scanners (Trivy, kube-bench) correctly identify vulnerabilities
    \item Users review patches before applying (system supports dry-run)
\end{itemize}

\subsection{Reproducibility}

All code, tests, fixtures, and dashboard are open-source:
\begin{itemize}
    \item Repository: \url{https://github.com/contact-shreyas/HardenerAI} (MIT License)
    \item Clone: \texttt{git clone https://github.com/contact-shreyas/HardenerAI.git}
    \item Build: \texttt{pnpm install \&\& pnpm build}
    \item Tests: \texttt{pnpm test} (21 tests, 100\% pass)
    \item Usage: \texttt{pnpm harden --target ./fixtures}
\end{itemize}

Docker image available for reproducible environments. CI/CD pipeline definition in \texttt{.github/workflows/ci.yml}. All dependencies are pinned in \texttt{pnpm-lock.yaml} for deterministic builds.

\section{Future Work}

\begin{enumerate}
    \item \textbf{ML-based scoring:} Train gradient-boosted decision trees or neural networks on CVE-to-config mappings to refine exploit likelihood estimates.
    
    \item \textbf{Context-aware patching:} Integrate with cloud provider APIs (AWS Config, Azure Policy) to fetch deployment context (production vs. staging, traffic volume) for adaptive recommendations.
    
    \item \textbf{Policy learning:} Implement reinforcement learning to optimize patch selection based on success/failure feedback from CI/CD pipelines.
    
    \item \textbf{Benchmark dataset:} Collaborate with community to create public configuration misconfiguration dataset with ground-truth risk labels.
    
    \item \textbf{Additional platforms:} Extend to Terraform, CloudFormation, Ansible playbooks, Prometheus alerting rules.
    
    \item \textbf{Real-time scanning:} Add webhook support for continuous monitoring (scan on Git push, K8s admission controller).
    
    \item \textbf{Formal verification:} Integrate tools like TLA+ or Alloy to prove safety properties of patches (e.g., ``Pod cannot escalate privileges'').
\end{enumerate}

\section{Conclusion}

We presented an AI-driven configuration hardening system addressing the SME challenge of managing infrastructure security with limited resources. Our CVSS-inspired composite risk scoring model, intelligent deduplication algorithm, and automated patch generation with validation/rollback capabilities deliver a production-ready pipeline reducing noise by 84\% and prioritizing findings with transparent, explainable risk scores (0--100 scale).

Evaluation on intentionally vulnerable test infrastructure validated the system's effectiveness: 98 raw findings consolidated to 16 unique issues, top risks correctly identified (45/100 for privileged pod with host network), and 40\% patch validation pass rate with actionable \texttt{TODO} guidance for incomplete fixes. The system integrates into CI/CD workflows via CLI and GitHub Actions, with a Next.js dashboard for visualization.

Key contributions include: (1) novel CVSS-adapted scoring for configurations, (2) 84\% noise reduction via hierarchical grouping, (3) safe patch automation with rollback, and (4) fully tested open-source implementation. Limitations include moderate patch completeness and static-only analysis; future work will incorporate ML-based scoring, policy learning, and expanded platform support.

This work demonstrates that SMEs can achieve enterprise-grade configuration security with lightweight, automated tooling. By reducing alert fatigue, providing transparent risk prioritization, and enabling safe remediation, we lower the barrier to infrastructure hardening for resource-constrained organizations.

\section*{Acknowledgments}
We thank the open-source community for tools (Trivy, kube-bench) and benchmarks (CIS) that enabled this research.

\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
